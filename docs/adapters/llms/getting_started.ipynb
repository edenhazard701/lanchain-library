{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939e05c5",
   "metadata": {},
   "source": [
    "# Getting Started \n",
    "\n",
    "This notebook demonstrates how to use Steamship compatible LLMs in LangChain app. \n",
    "Steamship compatible LLMs respect the LangChain's standard LLM interface and can easily be swapped by updating the import statement.\n",
    "Doing so will give you access to our distributed that scale with the number of requests. \n",
    "Since these LLMs are run in Steamship's cloud infrastructure you won't have to worry about installing upstream dependencies to run these LLMs. \n",
    "\n",
    "All our LLMs come preloaded with Api Keys. For more information about API Key management and billing, please read our access & billing section.\n",
    "\n",
    "More infromation about LangChain's LLM interface can be found [here](https://langchain.readthedocs.io/en/latest/modules/llms/getting_started.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8502a0d",
   "metadata": {},
   "source": [
    "## Supported LLM Providers\n",
    "\n",
    "Steamship supports 1 LLM provider. Support for more LLM providers is on its way. \n",
    "\n",
    "\n",
    "| Provider    | LangChain            | Steamship                       | \n",
    "|-------------|----------------------|---------------------------------|\n",
    "| OpenAI      | langchain.llms.OpenAI | steamship_langchain.llms.OpenAI |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550242a9",
   "metadata": {},
   "source": [
    "To use our adapter, import one of the supported LLMs from `steamship_langchain.llms` and connect it to an authenticated steamship client as follows: \n",
    "\n",
    "```diff\n",
    "- from langchain.llms import OpenAI\n",
    "+ from steamship_langchain.llms import OpenAI\n",
    "+ from steamship import Steamship\n",
    "\n",
    "+ client = Steamship()\n",
    "\n",
    "llm = OpenAI(\n",
    "+ client=client,\n",
    "  model_name=\"text-ada-001\", \n",
    "  n=2, \n",
    "  best_of=2,\n",
    "  temperature=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82ae9f3",
   "metadata": {},
   "source": [
    "## Example \n",
    "\n",
    "For this example, we will work with an OpenAI LLM wrapper, although the functionalities highlighted are generic for all LLM types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c108786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion: \n",
      "\n",
      "Why did the chicken cross the road?\n",
      "\n",
      "To get to the other side!\n"
     ]
    }
   ],
   "source": [
    "from steamship_langchain.llms import OpenAI\n",
    "from steamship import Steamship \n",
    "\n",
    "client = Steamship()\n",
    "\n",
    "llm = OpenAI(\n",
    "  client=client,\n",
    "  model_name=\"text-ada-001\", \n",
    "  n=2, \n",
    "  best_of=2,\n",
    "  temperature=0.9)\n",
    "\n",
    "print(\"Completion:\", llm(\"Tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b462ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00530113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_completions 30\n"
     ]
    }
   ],
   "source": [
    "print(\"n_completions\", len(llm_result.generations)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0246ae9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first completion [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!', generation_info=None), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info=None)]\n"
     ]
    }
   ],
   "source": [
    "print(\"first completion\", llm_result.generations[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c9d065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last completion [Generation(text=\"\\n\\nHow could I\\nMarry someone I don't love\\n\\nAnd be happy with them\\n\\nI'll find someone else to love\\n\\nI'll find someone else to love\\n\", generation_info=None), Generation(text=\"\\n\\nHow did you find your way\\n\\nTo a place like this?\\n\\nIt wasn't hard to find\\n\\nOnly took time and Location\\n\\nAnd I'm sure you'll find\\n\\nThat this is more than you could have ever hoped for\", generation_info=None)]\n"
     ]
    }
   ],
   "source": [
    "print(\"last completion\", llm_result.generations[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1424242a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_output {'token_usage': {'total_tokens': 4113, 'prompt_tokens': 120, 'completion_tokens': 3993}}\n"
     ]
    }
   ],
   "source": [
    "print(\"llm_output\", llm_result.llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a3620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens 3\n"
     ]
    }
   ],
   "source": [
    "print(\"number of tokens\", llm.get_num_tokens(\"what a joke\")) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
